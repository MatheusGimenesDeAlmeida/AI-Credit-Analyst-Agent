{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c251a014",
   "metadata": {},
   "source": [
    "# Market Sizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04846cd",
   "metadata": {},
   "source": [
    "### - Utilizando LLM para ler um demonstrativo financeiro de uma empresa e identificar o PDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2004d6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus Almeida\\OneDrive\\Documentos\\Insper\\Estágio\\AI-Credit-Analyst-Agent\\market_sizing\\venv\\Lib\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037f8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= CONFIGURAÇÕES =========\n",
    "load_dotenv()\n",
    "\n",
    "PDF_DIR = Path(\"../Demonstrativos\")\n",
    "INDEX_NAME = \"financial-reports\"\n",
    "CHUNK_SIZE = 1200\n",
    "CHUNK_OVERLAP = 200\n",
    "PINECONE_REGION = \"us-east-1\"\n",
    "BATCH_SIZE = 100  # Chunks por lote\n",
    "PDFS_PER_SESSION = 20  # Processa 20 PDFs por vez (segurança)\n",
    "# Para processar todos de uma vez: PDFS_PER_SESSION = None\n",
    "# =================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b4a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Sistema de Ingestão em Larga Escala\n",
      "============================================================\n",
      "\n",
      "📁 Total de PDFs na pasta: 58\n",
      "✅ Já processados: 40\n",
      "❌ Com falhas: 0\n",
      "📊 Total de chunks indexados: 13347\n",
      "🆕 Arquivos novos para processar: 18\n",
      "\n",
      "⚡ Processando 18 PDFs nesta sessão\n",
      "   (Restantes: 0)\n",
      "\n",
      "============================================================\n",
      "📄 [1/18] CAIXA SEGURIDADE.pdf\n",
      "============================================================\n",
      "📊 Gerados 358 chunks\n",
      "💰 Custo estimado: $0.0107\n",
      "  📦 Lote 1/4: 100 chunks... ✅\n",
      "  📦 Lote 2/4: 100 chunks... ✅\n",
      "  📦 Lote 3/4: 100 chunks... ✅\n",
      "  📦 Lote 4/4: 58 chunks... ✅\n",
      "✅ CAIXA SEGURIDADE.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [2/18] CAMIL.pdf\n",
      "============================================================\n",
      "📊 Gerados 341 chunks\n",
      "💰 Custo estimado: $0.0102\n",
      "  📦 Lote 1/4: 100 chunks... ✅\n",
      "  📦 Lote 2/4: 100 chunks... ✅\n",
      "  📦 Lote 3/4: 100 chunks... ✅\n",
      "  📦 Lote 4/4: 41 chunks... ✅\n",
      "✅ CAMIL.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [3/18] CBA.pdf\n",
      "============================================================\n",
      "📊 Gerados 354 chunks\n",
      "💰 Custo estimado: $0.0106\n",
      "  📦 Lote 1/4: 100 chunks... ✅\n",
      "  📦 Lote 2/4: 100 chunks... ✅\n",
      "  📦 Lote 3/4: 100 chunks... ✅\n",
      "  📦 Lote 4/4: 54 chunks... ✅\n",
      "✅ CBA.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [4/18] CCR.pdf\n",
      "============================================================\n",
      "📊 Gerados 451 chunks\n",
      "💰 Custo estimado: $0.0135\n",
      "  📦 Lote 1/5: 100 chunks... ✅\n",
      "  📦 Lote 2/5: 100 chunks... ✅\n",
      "  📦 Lote 3/5: 100 chunks... ✅\n",
      "  📦 Lote 4/5: 100 chunks... ✅\n",
      "  📦 Lote 5/5: 51 chunks... ✅\n",
      "✅ CCR.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [5/18] CELPE.pdf\n",
      "============================================================\n",
      "📊 Gerados 638 chunks\n",
      "💰 Custo estimado: $0.0191\n",
      "  📦 Lote 1/7: 100 chunks... ✅\n",
      "  📦 Lote 2/7: 100 chunks... ✅\n",
      "  📦 Lote 3/7: 100 chunks... ✅\n",
      "  📦 Lote 4/7: 100 chunks... ✅\n",
      "  📦 Lote 5/7: 100 chunks... ✅\n",
      "  📦 Lote 6/7: 100 chunks... ✅\n",
      "  📦 Lote 7/7: 38 chunks... ✅\n",
      "✅ CELPE.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [6/18] CEMIG.pdf\n",
      "============================================================\n",
      "📊 Gerados 795 chunks\n",
      "💰 Custo estimado: $0.0238\n",
      "  📦 Lote 1/8: 100 chunks... ✅\n",
      "  📦 Lote 2/8: 100 chunks... ✅\n",
      "  📦 Lote 3/8: 100 chunks... ✅\n",
      "  📦 Lote 4/8: 100 chunks... ✅\n",
      "  📦 Lote 5/8: 100 chunks... ✅\n",
      "  📦 Lote 6/8: 100 chunks... ✅\n",
      "  📦 Lote 7/8: 100 chunks... ✅\n",
      "  📦 Lote 8/8: 95 chunks... ✅\n",
      "✅ CEMIG.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [7/18] CENTAURO.pdf\n",
      "============================================================\n",
      "📊 Gerados 374 chunks\n",
      "💰 Custo estimado: $0.0112\n",
      "  📦 Lote 1/4: 100 chunks... ✅\n",
      "  📦 Lote 2/4: 100 chunks... ✅\n",
      "  📦 Lote 3/4: 100 chunks... ✅\n",
      "  📦 Lote 4/4: 74 chunks... ✅\n",
      "✅ CENTAURO.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [8/18] CIA BRASILEIRA DE DISTRIBUIÇÃO.pdf\n",
      "============================================================\n",
      "📊 Gerados 412 chunks\n",
      "💰 Custo estimado: $0.0124\n",
      "  📦 Lote 1/5: 100 chunks... ✅\n",
      "  📦 Lote 2/5: 100 chunks... ✅\n",
      "  📦 Lote 3/5: 100 chunks... ✅\n",
      "  📦 Lote 4/5: 100 chunks... ✅\n",
      "  📦 Lote 5/5: 12 chunks... ✅\n",
      "✅ CIA BRASILEIRA DE DISTRIBUIÇÃO.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [9/18] CIA FERRO LIGAS DA BAHIA.pdf\n",
      "============================================================\n",
      "📊 Gerados 304 chunks\n",
      "💰 Custo estimado: $0.0091\n",
      "  📦 Lote 1/4: 100 chunks... ✅\n",
      "  📦 Lote 2/4: 100 chunks... ✅\n",
      "  📦 Lote 3/4: 100 chunks... ✅\n",
      "  📦 Lote 4/4: 4 chunks... ✅\n",
      "✅ CIA FERRO LIGAS DA BAHIA.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [10/18] CIA LOCACOES DAS AMERICAS.pdf\n",
      "============================================================\n",
      "📊 Gerados 374 chunks\n",
      "💰 Custo estimado: $0.0112\n",
      "  📦 Lote 1/4: 100 chunks... ✅\n",
      "  📦 Lote 2/4: 100 chunks... ✅\n",
      "  📦 Lote 3/4: 100 chunks... ✅\n",
      "  📦 Lote 4/4: 74 chunks... ✅\n",
      "✅ CIA LOCACOES DAS AMERICAS.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [11/18] COELCE.pdf\n",
      "============================================================\n",
      "📊 Gerados 315 chunks\n",
      "💰 Custo estimado: $0.0095\n",
      "  📦 Lote 1/4: 100 chunks... ✅\n",
      "  📦 Lote 2/4: 100 chunks... ✅\n",
      "  📦 Lote 3/4: 100 chunks... ✅\n",
      "  📦 Lote 4/4: 15 chunks... ✅\n",
      "✅ COELCE.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [12/18] COMGAS.pdf\n",
      "============================================================\n",
      "📊 Gerados 153 chunks\n",
      "💰 Custo estimado: $0.0046\n",
      "  📦 Lote 1/2: 100 chunks... ✅\n",
      "  📦 Lote 2/2: 53 chunks... ✅\n",
      "✅ COMGAS.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [13/18] COPASA.pdf\n",
      "============================================================\n",
      "📊 Gerados 506 chunks\n",
      "💰 Custo estimado: $0.0152\n",
      "  📦 Lote 1/6: 100 chunks... ✅\n",
      "  📦 Lote 2/6: 100 chunks... ✅\n",
      "  📦 Lote 3/6: 100 chunks... ✅\n",
      "  📦 Lote 4/6: 100 chunks... ✅\n",
      "  📦 Lote 5/6: 100 chunks... ✅\n",
      "  📦 Lote 6/6: 6 chunks... ✅\n",
      "✅ COPASA.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [14/18] ENEL.pdf\n",
      "============================================================\n",
      "📊 Gerados 567 chunks\n",
      "💰 Custo estimado: $0.0170\n",
      "  📦 Lote 1/6: 100 chunks... ✅\n",
      "  📦 Lote 2/6: 100 chunks... ✅\n",
      "  📦 Lote 3/6: 100 chunks... ✅\n",
      "  📦 Lote 4/6: 100 chunks... ✅\n",
      "  📦 Lote 5/6: 100 chunks... ✅\n",
      "  📦 Lote 6/6: 67 chunks... ✅\n",
      "✅ ENEL.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [15/18] INTER.pdf\n",
      "============================================================\n",
      "📊 Gerados 232 chunks\n",
      "💰 Custo estimado: $0.0070\n",
      "  📦 Lote 1/3: 100 chunks... ✅\n",
      "  📦 Lote 2/3: 100 chunks... ✅\n",
      "  📦 Lote 3/3: 32 chunks... ✅\n",
      "✅ INTER.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [16/18] SABESP.pdf\n",
      "============================================================\n",
      "📊 Gerados 415 chunks\n",
      "💰 Custo estimado: $0.0125\n",
      "  📦 Lote 1/5: 100 chunks... ✅\n",
      "  📦 Lote 2/5: 100 chunks... ✅\n",
      "  📦 Lote 3/5: 100 chunks... ✅\n",
      "  📦 Lote 4/5: 100 chunks... ✅\n",
      "  📦 Lote 5/5: 15 chunks... ✅\n",
      "✅ SABESP.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [17/18] SANEPAR.pdf\n",
      "============================================================\n",
      "📊 Gerados 348 chunks\n",
      "💰 Custo estimado: $0.0104\n",
      "  📦 Lote 1/4: 100 chunks... ✅\n",
      "  📦 Lote 2/4: 100 chunks... ✅\n",
      "  📦 Lote 3/4: 100 chunks... ✅\n",
      "  📦 Lote 4/4: 48 chunks... ✅\n",
      "✅ SANEPAR.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📄 [18/18] SANTANDER.pdf\n",
      "============================================================\n",
      "📊 Gerados 588 chunks\n",
      "💰 Custo estimado: $0.0176\n",
      "  📦 Lote 1/6: 100 chunks... ✅\n",
      "  📦 Lote 2/6: 100 chunks... ✅\n",
      "  📦 Lote 3/6: 100 chunks... ✅\n",
      "  📦 Lote 4/6: 100 chunks... ✅\n",
      "  📦 Lote 5/6: 100 chunks... ✅\n",
      "  📦 Lote 6/6: 88 chunks... ✅\n",
      "✅ SANTANDER.pdf indexado com sucesso!\n",
      "\n",
      "============================================================\n",
      "📊 RESUMO DA SESSÃO\n",
      "============================================================\n",
      "⏱️  Tempo total: 11.0 minutos\n",
      "📄 PDFs processados: 18\n",
      "📊 Chunks indexados: 7525\n",
      "💰 Custo estimado: $0.2258\n",
      "\n",
      "🎯 TOTAL GERAL:\n",
      "   ✅ Processados: 58\n",
      "   📊 Total chunks: 20872\n",
      "   ❌ Falhas: 0\n",
      "\n",
      "🎉 TODOS OS PDFS FORAM PROCESSADOS!\n",
      "\n",
      "📝 Log salvo em: processing_log.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Arquivo de controle (rastreia o que já foi processado)\n",
    "TRACKING_FILE = Path(\"./processing_log.json\")\n",
    "\n",
    "# ========== CHAVES =========\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY or not PINECONE_API_KEY:\n",
    "    raise ValueError(\"API keys não definidas\")\n",
    "\n",
    "# ========== SISTEMA DE TRACKING ==========\n",
    "class ProcessingTracker:\n",
    "    \"\"\"Gerencia o histórico de processamento para evitar retrabalho\"\"\"\n",
    "    \n",
    "    def __init__(self, tracking_file: Path):\n",
    "        self.tracking_file = tracking_file\n",
    "        self.data = self._load()\n",
    "    \n",
    "    def _load(self) -> Dict:\n",
    "        \"\"\"Carrega histórico de processamento\"\"\"\n",
    "        if self.tracking_file.exists():\n",
    "            with open(self.tracking_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {\n",
    "            \"processed_files\": {},\n",
    "            \"failed_files\": {},\n",
    "            \"last_run\": None,\n",
    "            \"total_chunks\": 0\n",
    "        }\n",
    "    \n",
    "    def _save(self):\n",
    "        \"\"\"Salva histórico\"\"\"\n",
    "        with open(self.tracking_file, 'w') as f:\n",
    "            json.dump(self.data, f, indent=2)\n",
    "    \n",
    "    def is_processed(self, file_path: Path, file_hash: str) -> bool:\n",
    "        \"\"\"Verifica se arquivo já foi processado com mesmo conteúdo\"\"\"\n",
    "        filename = file_path.name\n",
    "        if filename in self.data[\"processed_files\"]:\n",
    "            return self.data[\"processed_files\"][filename][\"hash\"] == file_hash\n",
    "        return False\n",
    "    \n",
    "    def mark_processed(self, file_path: Path, file_hash: str, num_chunks: int):\n",
    "        \"\"\"Marca arquivo como processado\"\"\"\n",
    "        self.data[\"processed_files\"][file_path.name] = {\n",
    "            \"hash\": file_hash,\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"chunks\": num_chunks\n",
    "        }\n",
    "        self.data[\"total_chunks\"] += num_chunks\n",
    "        self._save()\n",
    "    \n",
    "    def mark_failed(self, file_path: Path, error: str):\n",
    "        \"\"\"Registra falha no processamento\"\"\"\n",
    "        self.data[\"failed_files\"][file_path.name] = {\n",
    "            \"error\": str(error),\n",
    "            \"failed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        self._save()\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Retorna estatísticas de processamento\"\"\"\n",
    "        return {\n",
    "            \"processed\": len(self.data[\"processed_files\"]),\n",
    "            \"failed\": len(self.data[\"failed_files\"]),\n",
    "            \"total_chunks\": self.data[\"total_chunks\"]\n",
    "        }\n",
    "\n",
    "# ========== FUNÇÕES AUXILIARES ==========\n",
    "def get_document_hash(pdf_path: Path) -> str:\n",
    "    \"\"\"Gera hash do PDF para detectar mudanças\"\"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Limpa e normaliza texto\"\"\"\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def identify_financial_sections(text: str) -> bool:\n",
    "    \"\"\"Identifica seções financeiras relevantes\"\"\"\n",
    "    keywords = [\n",
    "        'balanço patrimonial', 'ativo circulante', 'contas a receber',\n",
    "        'notas explicativas', 'instrumentos financeiros', 'provisões',\n",
    "        'clientes', 'duplicatas a receber', 'pcld', 'pdd'\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in keywords)\n",
    "\n",
    "def estimate_cost(num_chunks: int, avg_tokens_per_chunk: int = 300) -> float:\n",
    "    \"\"\"Estima custo de embedding (OpenAI ada-002: $0.0001/1k tokens)\"\"\"\n",
    "    total_tokens = num_chunks * avg_tokens_per_chunk\n",
    "    cost = (total_tokens / 1000) * 0.0001\n",
    "    return cost\n",
    "\n",
    "# ========== INICIALIZAÇÃO ==========\n",
    "print(\"🚀 Sistema de Ingestão em Larga Escala\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Cria índice se não existir\n",
    "existing = [i.name for i in pc.list_indexes()]\n",
    "if INDEX_NAME not in existing:\n",
    "    print(f\"Criando índice '{INDEX_NAME}'...\")\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec={\"serverless\": {\"cloud\": \"aws\", \"region\": PINECONE_REGION}}\n",
    "    )\n",
    "    time.sleep(10)  # Aguarda criação\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "tracker = ProcessingTracker(TRACKING_FILE)\n",
    "\n",
    "# ========== DESCOBERTA DE ARQUIVOS ==========\n",
    "if not PDF_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Pasta {PDF_DIR} não existe\")\n",
    "\n",
    "all_pdfs = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"\\n📁 Total de PDFs na pasta: {len(all_pdfs)}\")\n",
    "\n",
    "# Filtra PDFs já processados\n",
    "pdfs_to_process = []\n",
    "for pdf in all_pdfs:\n",
    "    file_hash = get_document_hash(pdf)\n",
    "    if not tracker.is_processed(pdf, file_hash):\n",
    "        pdfs_to_process.append(pdf)\n",
    "\n",
    "stats = tracker.get_stats()\n",
    "print(f\"✅ Já processados: {stats['processed']}\")\n",
    "print(f\"❌ Com falhas: {stats['failed']}\")\n",
    "print(f\"📊 Total de chunks indexados: {stats['total_chunks']}\")\n",
    "print(f\"🆕 Arquivos novos para processar: {len(pdfs_to_process)}\")\n",
    "\n",
    "if not pdfs_to_process:\n",
    "    print(\"\\n✨ Todos os arquivos já foram processados!\")\n",
    "    exit(0)\n",
    "\n",
    "# Limita quantidade por sessão para segurança\n",
    "if PDFS_PER_SESSION:\n",
    "    pdfs_this_session = pdfs_to_process[:PDFS_PER_SESSION]\n",
    "    print(f\"\\n⚡ Processando {len(pdfs_this_session)} PDFs nesta sessão\")\n",
    "    print(f\"   (Restantes: {len(pdfs_to_process) - len(pdfs_this_session)})\")\n",
    "else:\n",
    "    pdfs_this_session = pdfs_to_process\n",
    "    print(f\"\\n⚡ Processando TODOS os {len(pdfs_this_session)} PDFs nesta sessão\")\n",
    "\n",
    "# ========== PROCESSAMENTO ==========\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embeddings,\n",
    "    text_key=\"text\"\n",
    ")\n",
    "\n",
    "total_chunks_session = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, pdf_path in enumerate(pdfs_this_session, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📄 [{idx}/{len(pdfs_this_session)}] {pdf_path.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        company = pdf_path.stem\n",
    "        file_hash = get_document_hash(pdf_path)\n",
    "        \n",
    "        # Carrega e processa PDF\n",
    "        loader = PyPDFLoader(str(pdf_path))\n",
    "        pages = loader.load()\n",
    "        \n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        \n",
    "        for page_idx, page in enumerate(pages, start=1):\n",
    "            page_text = preprocess_text(page.page_content or \"\")\n",
    "            if not page_text:\n",
    "                continue\n",
    "            \n",
    "            is_relevant = identify_financial_sections(page_text)\n",
    "            chunks = splitter.split_text(page_text)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks, start=1):\n",
    "                texts.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"company\": company,\n",
    "                    \"source_file\": pdf_path.name,\n",
    "                    \"page\": page_idx,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"is_financial_section\": is_relevant,\n",
    "                    \"document_hash\": file_hash,\n",
    "                    \"processed_at\": datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        if not texts:\n",
    "            print(\"⚠️  Nenhum conteúdo extraído\")\n",
    "            tracker.mark_failed(pdf_path, \"No content extracted\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"📊 Gerados {len(texts)} chunks\")\n",
    "        \n",
    "        # Estima custo\n",
    "        cost = estimate_cost(len(texts))\n",
    "        print(f\"💰 Custo estimado: ${cost:.4f}\")\n",
    "        \n",
    "        # Indexa em lotes\n",
    "        total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        \n",
    "        for i in range(0, len(texts), BATCH_SIZE):\n",
    "            batch_texts = texts[i:i+BATCH_SIZE]\n",
    "            batch_metadatas = metadatas[i:i+BATCH_SIZE]\n",
    "            batch_num = (i // BATCH_SIZE) + 1\n",
    "            \n",
    "            print(f\"  📦 Lote {batch_num}/{total_batches}: {len(batch_texts)} chunks...\", end=\" \")\n",
    "            \n",
    "            try:\n",
    "                vectorstore.add_texts(\n",
    "                    texts=batch_texts,\n",
    "                    metadatas=batch_metadatas\n",
    "                )\n",
    "                print(\"✅\")\n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Erro: {e}\")\n",
    "                tracker.mark_failed(pdf_path, f\"Batch {batch_num} failed: {e}\")\n",
    "                raise\n",
    "        \n",
    "        # Marca como processado\n",
    "        tracker.mark_processed(pdf_path, file_hash, len(texts))\n",
    "        total_chunks_session += len(texts)\n",
    "        print(f\"✅ {pdf_path.name} indexado com sucesso!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERRO ao processar {pdf_path.name}: {e}\")\n",
    "        tracker.mark_failed(pdf_path, str(e))\n",
    "        continue\n",
    "\n",
    "# ========== RESUMO FINAL ==========\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"📊 RESUMO DA SESSÃO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"⏱️  Tempo total: {elapsed/60:.1f} minutos\")\n",
    "print(f\"📄 PDFs processados: {len(pdfs_this_session)}\")\n",
    "print(f\"📊 Chunks indexados: {total_chunks_session}\")\n",
    "print(f\"💰 Custo estimado: ${estimate_cost(total_chunks_session):.4f}\")\n",
    "\n",
    "final_stats = tracker.get_stats()\n",
    "print(f\"\\n🎯 TOTAL GERAL:\")\n",
    "print(f\"   ✅ Processados: {final_stats['processed']}\")\n",
    "print(f\"   📊 Total chunks: {final_stats['total_chunks']}\")\n",
    "print(f\"   ❌ Falhas: {final_stats['failed']}\")\n",
    "\n",
    "remaining = len(pdfs_to_process) - len(pdfs_this_session)\n",
    "if remaining > 0:\n",
    "    print(f\"\\n⏭️  Execute novamente para processar os {remaining} PDFs restantes\")\n",
    "    print(f\"   Estimativa: {remaining // PDFS_PER_SESSION + 1} sessões adicionais\")\n",
    "else:\n",
    "    print(f\"\\n🎉 TODOS OS PDFS FORAM PROCESSADOS!\")\n",
    "\n",
    "print(f\"\\n📝 Log salvo em: {TRACKING_FILE}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
